[TOC]

# Deep learning

## Use open source code

1. Use architectures of networks published in the literature
2. Use open source implementation if possible
3. Use pretrained models and fine-tune on your dataset

## ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ 

1. LSF

   ```pyhton
   J=0; dw1=0; dw2=0; db=0;
   for i = 1 to m
   z(i) = wx(i)+b;
   a(i) = sigmoid(z(i));
   J += -[y(i)log(a(i))+(1-y(i)ï¼‰log(1-a(i));
   dz(i) = a(i)-y(i);
   dw1 += x1(i)dz(i);
   dw2 += x2(i)dz(i);
   db += dz(i);
   J /= m;
   dw1 /= m;
   dw2 /= m;
   db /= m;
   w=w-alpha*dw
   b=b-alpha*db
                         
                         
   Vectorize implementation
    Z = np.dot(w.T,x)
    A = sigmoid(Z)
    dZ = A-Y
    dw = 1/m*X*(dz.T)
    db = 1/m*np.sum(dz)
    w = w - alpha*dw
    b = b - alpha*db                   
   ```

   ```python
   funtion_normalizeRow
   
       x_ = np.linalg.norm(x, axis=1, keepdims = True)
       
       x = x / x_
   
       return x
       
   
   ```
   
   A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (bâˆ—âˆ—câˆ—âˆ—d, a) is to use:
   
   ```python
   X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X
   
   ```
   
   > **What you need to remember:**
   >
   > Common steps for pre-processing a new dataset are:
   >
   > - Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
   > - Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)
   > - "Standardize" the data
   
2. NN

   

   ```python
   	dZ2 = A2 - Y
       dW2 = 1/m*dZ2.dot(A1.T)
       db2 = 1/m*np.sum(dZ2,axis = 1, keepdims = True)
       dZ1 = W2.T.dot(dZ2) * (1 - np.power(A1, 2))
       dW1 = dZ1.dot(X.T)
       db1 = 1/m*np.sum(dZ1, axis = 1, keepdims = True)
   ```

3. å¤šå±‚NN

   â€‹	Note that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. 

   <u>**Several layers have several activation functions**</u>, In deep learning, the "[LINEAR->ACTIVATION]" computation is counted as a single layer in the neural network, not two layers.

rule: Idea  to  Code to Experiment (circle)





## æ”¹å–„æ·±å±‚ç¥ç»ç½‘ç»œ

æ­£åˆ™åŒ–ï¼šæ—¨åœ¨æ¶ˆé™¤æ–¹å·®ä»¥åŠåå·®çš„ä¸€ç³»åˆ—æ–¹æ³•ï¼Œ åŒ…æ‹¬

1. æ•°æ®æ‰©å¢ï¼ˆç”¨äºä¿®æ­£æ–¹å·®ï¼‰
2. L2æ­£åˆ™åŒ–
3. dropoutæ­£åˆ™åŒ–ï¼ˆç”¨äºä¿®æ­£æ–¹å·®ï¼‰

#### ä¸€äº›éœ€è¦è€ƒè™‘çš„é—®é¢˜

> 1. å°æ•°æ®æ—¶ä»£ä¸ƒä¸‰åˆ†æˆ–è€…å…­äºŒäºŒåˆ†æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œåœ¨å¤§æ•°æ®æ—¶ä»£å„ä¸ªæ•°æ®é›†çš„æ¯”ä¾‹å¯èƒ½éœ€è¦å˜æˆ98%ï¼š1%ï¼š1%ï¼Œç”šè‡³è®­ç»ƒé›†çš„æ¯”ä¾‹æ›´å¤§ã€‚
>
> 2. #### <u>ç¡®ä¿éªŒè¯é›†å’Œæµ‹è¯•é›†æ¥è‡ªäºåŒä¸€åˆ†å¸ƒ</u>
>
> 3. æµ‹è¯•é›†çš„ç›®çš„æ˜¯å¯¹æœ€ç»ˆæ‰€é€‰å®šçš„ç¥ç»ç½‘ç»œç³»ç»Ÿåšå‡ºæ— åä¼°è®¡ï¼Œå¦‚æœä¸éœ€è¦æ— åä¼°è®¡ä¹Ÿå¯ä»¥ä¸è®¾ç½®æµ‹è¯•é›†ã€‚
>
> 4. æœºå™¨å­¦ä¹ æ¯”è¾ƒåœ¨æ„bias-variance trade-off,ä½†æ·±åº¦å­¦ä¹ çš„è¯¯å·®å¾ˆå°‘æƒè¡¡ä¸¤è€…ï¼Œæˆ‘ä»¬æ€»æ˜¯åˆ†åˆ«è®¨è®ºåå·®æˆ–è€…æ–¹å·®ï¼Œå´å¾ˆå°‘è°ˆåŠåå·®å’Œæ–¹å·®çš„æƒè¡¡é—®é¢˜
>
> 5. é‡‡ç”¨åŒè¾¹è¯¯å·®æ›´é€¼è¿‘å¯¼æ•°ï¼šä½¿ç”¨æ³°å‹’å®šç†æ¨å¯¼

2. è¯¦ç»†ä»‹ç»

   > 1. æ‰§è¡Œæ–¹é¢çš„å°å»ºè®®ï¼šä½¿ç”¨æ–°çš„costå‡½æ•°ï¼ŒåŒ…å«ç¬¬äºŒä¸ªæ­£åˆ™åŒ–é¡¹ã€‚
   >2. dropoutæœ¬è´¨ï¼šæ¯æ¬¡éƒ½è®­ç»ƒä¸åŒçš„å­é›†ï¼Œå‡æ‘Šå„ä¸ªæƒé‡ï¼Œä¸”åªä½¿ç”¨åœ¨è®­ç»ƒæœŸï¼Œå‰å‘å’Œåå‘éƒ½ä½¿ç”¨
   > 3. L2æ­£åˆ™åŒ–ï¼Œç¼ºç‚¹æ˜¯å¿…é¡»å°è¯•å¾ˆå¤šæ­£åˆ™åŒ–å‚æ•°Î»ï¼Œearly stopping çš„ä¼˜ç‚¹æ˜¯åªè¿è¡Œä¸€æ¬¡æ¢¯åº¦ä¸‹é™ï¼Œä½ å¯ä»¥æ‰¾å‡ºW çš„è¾ƒå°å€¼ï¼Œä¸­é—´å€¼å’Œè¾ƒå¤§å€¼ï¼Œï¼ˆå®é™…ä¸­ç”¨çš„ä¸å¤šï¼‰
   
3. ä½¿ç”¨æƒé‡åˆå§‹åŒ–è§£å†³çˆ†ç‚¸æˆ–è€…åå¡Œ

   > åŸå› ï¼šL>Iï¼Œäº§ç”Ÿexplodingï¼ŒL<1, äº§ç”Ÿvanish
   > $$
   > cost = W^{L-1}X
   > $$
   > å› æ­¤åˆå§‹æƒé‡å°½å¯èƒ½é€¼è¿‘1ï¼Œä»¥ä¸‹ä¸ºæ™®éæ³•åˆ™
   >
   > 1. å¦‚æœæ˜¯å¸¸ç”¨`relu`æ¿€æ´»å‡½æ•°ï¼Œ`W = np.random.randn(shape)*np.sqrt(2/n^(L-1))`
   > 2. å¦‚æœæ˜¯`tanh`ï¼Œ    `np.sqrt(1/n^(L-1)) or  np.sqrt(2/(n^(L-1) + n^L)`

4. æ¢¯åº¦æ£€éªŒæ³¨æ„äº‹é¡¹

   > 1. åªç”¨åœ¨debug
   > 2. å¸¦ä¸Šæ­£åˆ™åŒ–
   > 3. ä¸èƒ½ä¸dropoutåŒæ—¶ä½¿ç”¨
   > 4. åœ¨è¿™æ ·ä¸€ç§æƒ…å†µï¼šåªæœ‰Wå’Œbæ¥è¿‘0çš„æ—¶å€™BPæ‰æ˜¯æ­£ç¡®çš„ã€‚åœ¨éšæœºåˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œè¿è¡Œæ¢¯åº¦æ£€éªŒï¼Œç„¶åå†è®­ç»ƒç½‘ç»œ

#### dropout å’ŒL2æ­£åˆ™åŒ–ä½¿ç”¨ä»£ç 

```python
# Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        if keep_prob == 1:
            a3, cache = forward_propagation(X, parameters)
        elif keep_prob < 1:
            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)
        
        # Cost function
        if lambd == 0:
            cost = compute_cost(a3, Y)
        else:
            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)
            
        # Backward propagation.
        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, 
                                            # but this assignment will only explore one at a time
        if lambd == 0 and keep_prob == 1:
            grads = backward_propagation(X, Y, cache)
        elif lambd != 0:
            grads = backward_propagation_with_regularization(X, Y, cache, lambd)
        elif keep_prob < 1:
            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)
            
            
               
            
            FORWARD_PROPAGATION
    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. 
    D1 = np.random.rand(A1.shape[0], A1.shape[1])                                         # Step 1: initialize matrix D1 = np.random.rand(..., ...)
    D1 = D1 < keep_prob                                         # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)
    A1 = D1 * A1                                         # Step 3: shut down some neurons of A1
    A1 = A1 / keep_prob                                         # Step 4: scale the value of neurons
    
    
    
    		BACKWARD_PROPAGATION
     ### START CODE HERE ### (â‰ˆ 2 lines of code)
    dA2 = D2 * dA2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
    dA2 = dA2 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut dow
```

#### æ¢¯åº¦æ£€éªŒä»£ç 

```python
To compute J_plus[i]:
    Set Î¸+ to np.copy(parameters_values)
    Set Î¸+i to Î¸+i+Îµ
    Calculate J+i using to forward_propagation_n(x, y, vector_to_dictionary(Î¸+ )).
To compute J_minus[i]: do the same thing with Î¸âˆ’
Compute gradapprox[i]=J+iâˆ’Jâˆ’i2Îµ

    for i in range(num_parameters): 
        
        # Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".
        # "_" is used because the function you have to outputs two parameters but we only care about the first one
        ### START CODE HERE ### (approx. 3 lines)
        theta_plus = np.copy(parameters_values)
        theta_plus[i][0] += epsilon
        J_plus[i], _ = forward_propagation_n(X, Y ,vector_to_dictionary(theta_plus))
        ### END CODE HERE ###
        
        # Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".
        ### START CODE HERE ### (approx. 3 lines)
        theta_minus = np.copy(parameters_values)
        theta_minus[i][0] -= epsilon
        J_minus[i], _ = forward_propagation_n(X, Y ,vector_to_dictionary(theta_minus))
        ### END CODE HERE ###
        
        # Compute gradapprox[i]
        ### START CODE HERE ### (approx. 1 line)
        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)
        ### END CODE HERE ###
    
    # Compare gradapprox to backward propagation gradients by computing difference.
    ### START CODE HERE ### (approx. 1 line)
    numerator = np.linalg.norm(grad - gradapprox) 
    denominator = np.linalg.norm(gradapprox) + np.linalg.norm(grad)
    difference = numerator / denominator
    ### END CODE HERE ###
```

#### ä¼˜åŒ–ç®—æ³•

1. mini-batch

   1. æ ·æœ¬é›†(m<=2000)ï¼Œä½¿ç”¨batchæ¢¯åº¦ä¸‹é™
   2. mini-batchå°è¯•å‡ ä¸ªä¸åŒçš„2æ¬¡æ–¹ï¼Œä¸€èˆ¬ä¸º64åˆ°512ï¼Œè¿™æ˜¯è€ƒè™‘åˆ°CPU/GPUå†…å­˜ç›¸ç¬¦

   ç»æµ‹è¯•ï¼Œæ‰¹æ¬¡é€‰æ‹©16æˆ–è€…8æœ€å¥½

3. Gradient descent with momentum
   
   åŠ¨é‡ç®—æ³•ä¸ºè§£å†³å¦‚ä¸‹æƒ…å†µè€Œå‡ºç°çš„ï¼Œé€šè¿‡å¹³å‡è¿‡å»çš„å¤šå°‘å¤©ï¼ˆç”±Î²è®¾å®šï¼‰æŠµæ¶ˆç«–ç›´æ–¹å‘çš„æ‘†åŠ¨ï¼Œè€Œæ°´å¹³æ–¹å‘æŒç»­å¢é•¿ã€‚
   
   åŠ¨é‡ç®—æ³•éœ€è¦å°½å¯èƒ½å¤§ä¸€ç‚¹çš„å­¦ä¹ ç‡å’Œå¤æ‚ä¸€ç‚¹çš„æ•°æ®
   
   ![1559118326188](E:\local\deep\deep_learning\Deep learning.assets\1559118326188.png)
   $$
\begin{align}
   v_{dw} = {\beta}v_{dw} \ +\  (1 - \beta)dW\\
   v_{db} = \beta v_{db} \ + \ (1-\beta)db\\
   W = W\ -\ \alpha v_{dv}, \ \ b = b\ - \ \alpha v_{db}
   \end{align}
   $$
   
3. RMSprop------root mean square prop

   åŸç†ä¸åŠ¨é‡ç®—æ³•ä¸€è‡´ï¼Œè®¡ç®—æœ‰æ‰€åŒºåˆ«![1559118343252](E:\local\deep\deep_learning\Deep learning.assets\1559118343252.png)

4. Adam-----Adaptive Moment Estimation

   ç»“åˆäº†ä»¥ä¸Šä¸¤ç§ç®—æ³•

   ![1559118079574](E:\local\deep\deep_learning\Deep learning.assets\1559118079574.png)

   é€šå¸¸ï¼Œ Î²1å–0.9ï¼Œ Î²2å–0.999ï¼Œ Îµå–10e-8

#### learning rate decayï¼ˆè¡°å‡ç‡ï¼‰

#### å±€éƒ¨æœ€ä¼˜é—®é¢˜

1. å®é™…ä¸Šé«˜ç»´ç©ºé—´åŸºæœ¬ä¸Šä¸å­˜åœ¨å±€éƒ¨æœ€ä¼˜ï¼ˆæ‰€æœ‰éƒ½æ˜¯åŒä¸€ä¸ªæ–¹å‘ï¼‰
2. çœŸæ­£çš„é—®é¢˜æ˜¯ç¼“æ…¢èµ°è¿‡å¹³ç¨³æ®µï¼Œè¿™ä¹Ÿæ˜¯RMSpropã€Momentum å’ŒAdam èƒ½å¤ŸåŠ é€Ÿå­¦ä¹ ç®—æ³•çš„åœ°æ–¹ã€‚

#### å®ç°ä»£ç 

1. æ¢¯åº¦

   ```python
   (Batch) Gradient Descent:
   X = data_input
   Y = labels
   parameters = initialize_parameters(layers_dims)
   for i in range(0, num_iterations):
       # Forward propagation
       a, caches = forward_propagation(X, parameters)
       # Compute cost.
       cost = compute_cost(a, Y)
       # Backward propagation.
       grads = backward_propagation(a, caches, parameters)
       # Update parameters.
       parameters = update_parameters(parameters, grads)
   Stochastic Gradient Descent:
   X = data_input
   Y = labels
   parameters = initialize_parameters(layers_dims)
   for i in range(0, num_iterations):
       for j in range(0, m):
           # Forward propagation
           a, caches = forward_propagation(X[:,j], parameters)
           # Compute cost
           cost = compute_cost(a, Y[:,j])
           # Backward propagation
           grads = backward_propagation(a, caches, parameters)
           # Update parameters.
           parameters = update_parameters(parameters, grads)
   ```

2. æ‰¹é‡

   ```python
   # GRADED FUNCTION: random_mini_batches
   
   def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
       """
       Creates a list of random minibatches from (X, Y)
       
       Arguments:
       X -- input data, of shape (input size, number of examples)
       Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
       mini_batch_size -- size of the mini-batches, integer
       
       Returns:
       mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
       """
       
       np.random.seed(seed)            # To make your "random" minibatches the same as ours
       m = X.shape[1]                  # number of training examples
       mini_batches = []
           
       # Step 1: Shuffle (X, Y)
       permutation = list(np.random.permutation(m))
       shuffled_X = X[:, permutation]
   
       shuffled_Y = Y[:, permutation].reshape((1,m))
   
       # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
       num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
       for k in range(0, num_complete_minibatches):
           ### START CODE HERE ### (approx. 2 lines)
           mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]
           mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]
           ### END CODE HERE ###
           mini_batch = (mini_batch_X, mini_batch_Y)
           mini_batches.append(mini_batch)
       
       # Handling the end case (last mini-batch < mini_batch_size)
       if m % mini_batch_size != 0:
           ### START CODE HERE ### (approx. 2 lines)
           mini_batch_X = shuffled_X[:, num_compl ete_minibatches * mini_batch_size:]
           mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]
           ### END CODE HERE ###
           mini_batch = (mini_batch_X, mini_batch_Y)
           mini_batches.append(mini_batch)
       
       return mini_batches
   ```
   
   è‡³äºåŠ¨é‡å’ŒAdamç®—æ³•åªéœ€ä¿®æ”¹è®¡ç®—æ¢¯åº¦çš„å…¬å¼å³å¯ã€‚

#### è°ƒè¯•è¶…å‚æ•°

1. ç”±ç²—åˆ°ç»†ï¼Œè°ƒå‚ä¼˜å…ˆçº§é¡ºåºï¼šç¬¬ä¸€ï¼šÎ±ï¼Œ ç¬¬äºŒï¼šlayerï¼Œ learning rate decay ç¬¬ä¸‰ï¼šÎ²ï¼Œ hidden_units,  mini-batch size
2. åæ ‡é€‰å–èŒƒå›´å…ˆæ˜¯ç”¨å¯¹æ•°ã€‚ç„¶ååœ¨ç²¾ç»†
3. è¶…å‚æ•°è®­ç»ƒçš„æ–¹å¼ï¼š1ã€ç†ŠçŒ«æ–¹å¼       2ã€é±¼å­é…±æ–¹å¼ï¼ˆä¼˜å…ˆ,å¦‚æœæ•°æ®é‡è¶³å¤Ÿå¤§ï¼‰
4. batchå½’ä¸€åŒ–ï¼ˆåº”ç”¨äºZå’Œaä¹‹é—´ï¼‰ï¼Œç”±äºéœ€è¦å‡å»å‡å€¼ï¼Œæ‰€ä»¥bæ²¡æœ‰æ„ä¹‰

## Tessorflow åŸºç¡€

> 1. #### create placeholders
>
>    ```python
>    X = tf.placeholder(tf.float32, (n_x, None))
>    Y = tf.placeholder(tf.float32, (n_y, None))
>    ```
>
> 2. #### Initializing the parameters
>
>    ```python
>        tf.set_random_seed(1)                   # so that your "random" numbers match ours
>            
>        ### START CODE HERE ### (approx. 6 lines of code)
>        W1 = tf.get_variable("W1", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))
>        b1 = tf.get_variable("b1", [25,1], initializer = tf.zeros_initializer())
>    ```
>
> 3. #### Forward propagation in tensorflow
>
>    ```python
>        Z1 = tf.matmul(W1,X) + b1                                              # Z1 = np.dot(W1, X) + b1
>        A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)
>        Z2 = tf.matmul(W2,A1) + b2                                              # Z2 = np.dot(W2, a1) + b2
>        A2 = tf.nn.relu(Z2)                                          # A2 = relu(Z2)
>        Z3 = tf.matmul(W3,A2) + b3   
>     you don't need a3
>    ```
>
> 4. #### Compute cost
>
>    ```
>    It is important to know that the "logits" and "labels" inputs of tf.nn.softmax_cross_entropy_with_logits are expected to be of shape (number of examples, num_classes). We have thus transposed Z3 and Y for you.
>    Besides, tf.reduce_mean basically does the summation over the examples.   
>       
>       
>       # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)
>            logits = tf.transpose(Z3)
>        labels = tf.transpose(Y)
>        
>        ### START CODE HERE ### (1 line of code)
>        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))
>    
>    ```
>
> 5. #### Backward propagation & parameter updates
>
>    ```
>    For instance, for gradient descent the optimizer would be:
>    
>    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)
>    To make the optimization you would do:
>    
>    _ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})
>    ```
>
> 6. #### Building the model
>
>    ```
>    def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,
>              num_epochs = 1500, minibatch_size = 32, print_cost = True):
>    ```
>
>    
>
> 7. #### Test with your own image
>
> 8. #### ```,optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost),To make the optimization you would do:,,_ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y}),```,,
>
> 
>
> 
>
> 
>
> 
>
> 
>
> 

## ç»“æ„åŒ–æœºå™¨å­¦ä¹ é¡¹ç›®

æ­£äº¤åŒ–ç”¨äºè°ƒå‚

æŸ¥å…¨ç‡ä¸æŸ¥å‡†ç‡

æˆ‘ä»¬å¸Œæœ›é€šè¿‡åœ¨<u>**åŒä¸€åˆ†å¸ƒä¸­è®¾ç«‹å¼€å‘é›†å’Œæµ‹è¯•é›†**</u>ï¼Œä½ å°±å¯ä»¥ç„å‡†ä½ æ‰€å¸Œæœ›çš„æœºå™¨å­¦ä¹ å›¢é˜Ÿç„å‡†çš„ç›®æ ‡ã€‚

#### è¯¯å·®åˆ†æ

ç³»ç»Ÿæ€§çš„é”™è¯¯éœ€è¦çº æ­£

![1559135739971](E:\local\deep\deep_learning\Deep learning.assets\1559135739971.png)

è®­ç»ƒæ•°æ®å’Œå¼€å‘æ•°æ®æ¥è‡ªä¸åŒçš„åˆ†å¸ƒï¼š

![1559136155851](E:\local\deep\deep_learning\Deep learning.assets\1559136155851.png)

![1559136804022](E:\local\deep\deep_learning\Deep learning.assets\1559136804022.png)

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¸åŒçš„æ ·æœ¬ï¼Œå‡è®¾è®­ç»ƒè¯¯å·®ä¸º1%ï¼Œè®­ç»ƒ-å¼€å‘è¯¯å·®ä¸º1.5%ï¼Œä½†å½“ä½ å¼€å§‹å¤„ç†å¼€å‘é›†æ—¶ï¼Œé”™è¯¯ç‡ä¸Šå‡åˆ°10%ã€‚è¿™å°±æ˜¯æ•°æ®ä¸åŒ¹é…ï¼ˆæœ‰å‰æï¼‰

##### å¤„ç†æ•°æ®ä¸åŒ¹é…é—®é¢˜

![1559137454632](E:\local\deep\deep_learning\Deep learning.assets\1559137454632.png)

#### è¿ç§»å­¦ä¹ ï¼ˆTransfer learningï¼‰

![1559179037673](E:\local\deep\deep_learning\Deep learning.assets\1559179037673.png)

#### å¤šä»»åŠ¡å­¦ä¹ ï¼ˆmulti-taskï¼‰

![1559179147265](E:\local\deep\deep_learning\Deep learning.assets\1559179147265.png)

#### ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ ï¼ˆend-to-end learning)

![1559179294930](E:\local\deep\deep_learning\Deep learning.assets\1559179294930.png)

![1559179331315](E:\local\deep\deep_learning\Deep learning.assets\1559179331315.png)

![1559179366306](E:\local\deep\deep_learning\Deep learning.assets\1559179366306.png)



## å·ç§¯ç¥ç»ç½‘ç»œ

why:

1. Parameter compression
2. Sparsity(ç¨€ç–)of connections

paper:AlexNet, VGG, LeNet

paddingå…¬å¼ï¼š
$$
\frac{n-2*pad-f}s + 1 = n^{'}
\\
n^{'}æ˜¯å¡«å……ä¸å¦åçš„ç»´åº¦
$$

#### æ®‹å·®ç½‘ç»œï¼ˆResNets)ï¼ˆResidual Networks)

è§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚

è·³è·ƒè¿æ¥ï¼ˆSkip connectionï¼‰ï¼šå®ƒå¯ä»¥ä»æŸä¸€å±‚ç½‘ç»œå±‚è·å–æ¿€æ´»ï¼Œç„¶åè¿…é€Ÿåé¦ˆç»™å¦å¤–ä¸€å±‚ï¼Œç”šè‡³æ˜¯ç¥ç»ç½‘ç»œçš„æ›´æ·±å±‚ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è·³è·ƒè¿æ¥æ„å»ºèƒ½å¤Ÿè®­ç»ƒæ·±åº¦ç½‘ç»œçš„ResNetsï¼Œæœ‰æ—¶æ·±åº¦èƒ½å¤Ÿè¶…è¿‡100 å±‚ï¼Œ

#### 1x1å·ç§¯

æœ€å¼€å§‹ç”¨å·ç§¯è®¡ç®—çš„ç›®çš„å°±æ˜¯è¾ƒå°‘è®¡ç®—é‡ï¼Œè€Œå·ç§¯æ ¸æ˜¯è¶Šå°è¶Šå¥½ï¼Œæ¯”å¦‚ä¸€å¼ 1000x1000çš„å›¾ç‰‡ï¼Œå¦‚æœå·ç§¯æ ¸ä¸º1000ï¼Œé‡‡ç”¨1ä¸ªæ ¸å°±èƒ½æè¿°å›¾ç‰‡ï¼Œé‚£ä¹ˆè®¡ç®—é‡ä¸º1000x1000x1ï¼Œ å¦‚æœå·ç§¯æ ¸ä¸º10ï¼Œé‚£ä¹ˆé‡‡ç”¨å‡ åä¸ªå°±è¶³ä»¥å¤ç°å›¾ç‰‡ï¼Œè®¡ç®—é‡ä¸è¿‡1000ä»¥ä¸‹ï¼Œä¸”å·ç§¯æ ¸çš„ä¸ªæ•°è¶Šå¤šï¼Œæ‰€èƒ½æè¿°çš„ç»†èŠ‚è¶Šå¤šï¼Œä»è€Œéµå¾ªå°è€Œæ·±ã€‚

é€šè¿‡1Ã—1 å·ç§¯çš„ç®€å•æ“ä½œæ¥å‹ç¼©æˆ–ä¿æŒè¾“å…¥å±‚ä¸­çš„é€šé“æ•°é‡ï¼Œç”šè‡³æ˜¯å¢åŠ é€šé“æ•°é‡ã€‚

##### Inception

Inception ç½‘ç»œæˆ–Inception å±‚çš„ä½œç”¨å°±æ˜¯ä»£æ›¿äººå·¥æ¥ç¡®å®šå·ç§¯å±‚ä¸­çš„è¿‡æ»¤å™¨ç±»å‹ï¼Œæˆ–è€…ç¡®å®šæ˜¯å¦éœ€è¦åˆ›å»ºå·ç§¯å±‚æˆ–æ± åŒ–å±‚ï¼ˆåŸç†æ˜¯é€šè¿‡è®¡ç®—é‡çš„å¤šå°‘åˆ¤æ–­ï¼‰.

![1559180798247](E:\local\deep\deep_learning\Deep learning.assets\1559180798247.png)

![1559180810845](E:\local\deep\deep_learning\Deep learning.assets\1559180810845.png)

#### Common Data augmentation method

1. Mirroring
2. Random Cropping
3. Rotation shearing Local warping(use very little)

Color shifting

â€‹	å¯¹ç…§ç‰‡çš„é¢œè‰²æ›´æ”¹æ›´å…·é²æ£’æ€§ 

â€‹    paperï¼šmethod:PCA:AlexNet "PCA color augumentation"



#### Target Detection

YOLOç®—æ³•



![1559182431560](E:\local\deep\deep_learning\Deep learning.assets\1559182431560.png)

![](E:\local\deep\deep_learning\Deep learning.assets\flatten.png)

![](E:\local\deep\deep_learning\Deep learning.assets\probability_extraction.png)

![1559182113944](E:\local\deep\deep_learning\Deep learning.assets\1559182113944.png)

> paperï¼šYou Only Look Onceï¼šUnified real-time object detectionï¼ˆéš¾åº¦é«˜ï¼‰
>
> IOUï¼ˆintersection over unionï¼‰äº¤å¹¶æ¯”
>
> Non-max suppression 
>
> Anchor boxï¼šæ£€æµ‹ä¸€ä¸ªæ ¼å­å†…å¤šç›®æ ‡
>
> propose regionï¼ˆå€™é€‰åŒºåŸŸï¼‰

##### Yoloå®ç°ä»£ç 

```python
def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):
    """Filters YOLO boxes by thresholding on object and class confidence.
    
    Arguments:
    box_confidence -- tensor of shape (19, 19, 5, 1)
    boxes -- tensor of shape (19, 19, 5, 4)
    box_class_probs -- tensor of shape (19, 19, 5, 80)
    threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box
    
    Returns:
    scores -- tensor of shape (None,), containing the class probability score for selected boxes
    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes
    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes
    
    Note: "None" is here because you don't know the exact number of selected boxes, as it depends on the threshold. 
    For example, the actual output size of scores would be (10,) if there are 10 boxes.
    """
    
    # Step 1: Compute box scores
    
    scores = box_confidence * box_class_probs # æ¯ä¸ªcellé‡Œçš„å„ç§åˆ†ç±»çš„æ¦‚ç‡
    

    # Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score
    
    index = K.argmax(scores, axis = -1)    # ç»´åº¦åœ¨å€’æ•°ç¬¬ä¸€å„ç§åˆ†ç±»æ¦‚ç‡é‡Œçš„æœ€å¤§å€¼çš„ç´¢å¼•
    box_class_scores = K.max(index, axis = -1, keepdims = False) # 5ä¸ªanchorä¸­å€¼æœ€å¤§çš„cellå€¼
    
    
    
    # Step 3: Create a filtering mask based on "box_class_scores" by using "threshold". The mask should have the
    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability >= threshold)
    
    
    mask = box_class_scores >= threshold
    
    ### END CODE HERE ###
    
    # Step 4: Apply the mask to scores, boxes and classes
    
    scores = tf.boolean_mask(box_class_scores, mask)
    boxes = tf.boolean_mask(boxes, mask)
    classes = tf.boolean_mask(box_classes, maskï¼‰
    
    return scores, boxes, classes
                              
                              
                              
                              
                              
                              
                              
                              
                              
                              
                              
 # äº¤å¹¶æ¯”
 def iou(box1, box2):
    """Implement the intersection over union (IoU) between box1 and box2
    # å·¦ä¸‹ å³ä¸Š
    Arguments:
    box1 -- first box, list object with coordinates (x1, y1, x2, y2)
    box2 -- second box, list object with coordinates (x1, y1, x2, y2)
    """

    # Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area.
    ### START CODE HERE ### (â‰ˆ 5 lines)
    xi1 = max(box1[0], box2[0])
    yi1 = max(box1[1], box2[1])
    xi2 = min(box1[2], box2[2])
    yi2 = min(box1[3], box2[3])
    inter_area = (yi2 - yi1) * (xi2 - xi1)

    
    ### END CODE HERE ###    

    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)
    ### START CODE HERE ### (â‰ˆ 3 lines)
    box1_area = (box1[3] - box1[1])*(box1[2] - box1[0]) 
    box2_area = (box2[3] - box2[1])*(box2[2] - box2[0])
    union_area = box1_area + box2_area - inter_area
    
    ### END CODE HERE ###
    
    # compute the IoU
    ### START CODE HERE ### (â‰ˆ 1 line)
    iou = inter_area / union_area
    ### END CODE HERE ###

    return iou
                              
    
                              
                              
                              
                              
                              
                              
                              
                              
 # éæœ€å¤§å€¼æŠ‘åˆ¶
 
def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):
    """
    Applies Non-max suppression (NMS) to set of boxes
    
    Arguments:
    scores -- tensor of shape (None,), output of yolo_filter_boxes()
    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)
    classes -- tensor of shape (None,), output of yolo_filter_boxes()
    max_boxes -- integer, maximum number of predicted boxes you'd like
    iou_threshold -- real value, "intersection over union" threshold used for NMS filtering
    
    Returns:
    scores -- tensor of shape (, None), predicted score for each box
    boxes -- tensor of shape (4, None), predicted box coordinates
    classes -- tensor of shape (, None), predicted class for each box
    
    Note: The "None" dimension of the output tensors has obviously to be less than max_boxes. Note also that this
    function will transpose the shapes of scores, boxes, classes. This is made for convenience.
    """
    
    max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()
    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor
    
    # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep
    ### START CODE HERE ### (â‰ˆ 1 line)
    
    index = tf.image.non_max_suppression(boxes,scores,max_boxes,iou_threshold=0.5)
    
    ### END CODE HERE ###
    
    # Use K.gather() to select only nms_indices from scores, boxes and classes
    ### START CODE HERE ### (â‰ˆ 3 lines)
    scores = tf.gather(scores,index,axis=0)
    boxes = tf.gather(boxes, index, axis = 0)
    classes = tf.gather(classes, index, axis = 0)
    ### END CODE HERE ###
    
    return scores, boxes, classes
                              
                              
                              
                              
                              
                              
 # é¢„æµ‹
 def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):
    """
    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.
    
    Arguments:
    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:
                    box_confidence: tensor of shape (None, 19, 19, 5, 1)
                    box_xy: tensor of shape (None, 19, 19, 5, 2)
                    box_wh: tensor of shape (None, 19, 19, 5, 2)
                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)
    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)
    max_boxes -- integer, maximum number of predicted boxes you'd like
    score_threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box
    iou_threshold -- real value, "intersection over union" threshold used for NMS filtering
    
    Returns:
    scores -- tensor of shape (None, ), predicted score for each box
    boxes -- tensor of shape (None, 4), predicted box coordinates
    classes -- tensor of shape (None,), predicted class for each box
    """
    
    ### START CODE HERE ### 
    
    # Retrieve outputs of the YOLO model (â‰ˆ1 line)
    box_confidence, box_xy, box_wh, box_class_probx = yolo_outputs

    # Convert boxes to be ready for filtering functions 
    boxes = yolo_boxes_to_corners(box_xy, box_wh) 

    # Use one of the functions you've implemented to perform Score-filtering with a threshold of score_threshold (â‰ˆ1 line)
    scores, boxes, classes = yolo_filter_boxes(box_confidence, 
                                               boxes,box_class_probs, 
                                               score_threshold)

    # Scale boxes back to original image shape.
    boxes = scale_boxes(boxes, image_shape)

    # Use one of the functions you've implemented to perform Non-max suppression with a threshold of iou_threshold (â‰ˆ1 line)
    scores, boxes, classes = yolo_non_max_suppression(scores, 
                                                      boxes, classes,
                                                      max_boxes, iou_threshold)
    
    ### END CODE HERE ###
    
    return scores, boxes, classes                             
```

#### è„¸éƒ¨è¯†åˆ«

Method 

1. Given 3 image A, P, Nï¼š
   $$
   L(A, P, N) = max(||f(A) - f(P)||^2\ - \ ||f(A) - f(N)||^2 \ + \alpha\ , 0 )
   $$
   Training set: 10K pictures of 1k persons

   Paper:2015, FaceNet:A unified embedding for face recognition and clustering

2. Similarity function

   input
   $$
   |f(x^{(i)})_k - f(x^{(j)})_k|
   $$
   create LS two classification ,one of the input uses percomputing

   paper:2014, DeepFace closing the gap to human level performance



#### Neural style transfer

paper:2015, A neural algorithm of artistic style

 To run an image through this network, you just have to feed the image to the model. In TensorFlow, you can do so using the [tf.assign](https://www.tensorflow.org/api_docs/python/tf/assign) function. In particular, you will use the assign function like this:

```python
model["input"].assign(image)
```

This assigns the image as an input to the model. After this, if you want to access the activations of a particular layer, say layer `4_2` when the network is run on this image, you would run a TensorFlow session on the correct tensor `conv4_2`, as follows:

```python
sess.run(model["conv4_2"])
```

> 1. Create an Interactive Session
> 2. Load the content image 
> 3. Load the style image
> 4. Randomly initialize the image to be generated 
> 5. Load the VGG16 model
> 7. Build the TensorFlow graph:
>     - Run the content image through the VGG16 model and compute the content cost
>     - Run the style image through the VGG16 model and compute the style cost
>     - Compute the total cost
>     - Define the optimizer and the learning rate
> 8. Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step.
>

1.

```python
# Reset the graph
tf.reset_default_graph()

# Start interactive session
sess = tf.InteractiveSession()
```

2.

```python
import imageio
content_image = imageio.imread("images/louvre_small.jpg")
content_image = reshape_and_normalize_image(content_image)
```

3.

```
style_image = imageio.imread("images/monet.jpg")
style_image = reshape_and_normalize_image(style_image)
```

4.

```python
generated_image = generate_noise_image(content_image)
imshow(generated_image[0])
```

5:load the VGG16 model

```python
model = load_vgg_model("pretrained-model/imagenet-vgg-verydeep-19.mat")
```

6:omit some details

```python
# Assign the content image to be the input of the VGG model.  
sess.run(model['input'].assign(content_image))

# Select the output tensor of layer conv4_2
out = model['conv4_2']

# Set a_C to be the hidden layer activation from the layer we have selected
a_C = sess.run(out)

# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] 
# and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that
# when we run the session, this will be the activations drawn from the appropriate layer, with G as input.
a_G = out

# Compute the content cost
J_content = compute_content_cost(a_C, a_G)
```

7.

```python
def model_nn(sess, input_image, num_iterations = 200):
    
    # Initialize global variables (you need to run the session on the initializer)
    ### START CODE HERE ### (1 line)
    sess.run(tf.global_variables_initializer())
    ### END CODE HERE ###
    
    # Run the noisy input image (initial generated image) through the model. Use assign().
    ### START CODE HERE ### (1 line) 
    sess.run(model['input'].assign(input_image))
    ### END CODE HERE ###
    
    for i in range(num_iterations):
    
        # Run the session on the train_step to minimize the total cost
        ### START CODE HERE ### (1 line)
        sess.run(train_step)
        ### END CODE HERE ###
        
        # Compute the generated image by running the session on the current model['input']
        ### START CODE HERE ### (1 line)
        generated_image = sess.run(model['input'])
        ### END CODE HERE ###

        # Print every 20 iteration.
        if i%20 == 0:
            Jt, Jc, Js = sess.run([J, J_content, J_style])
            print("Iteration " + str(i) + " :")
            print("total cost = " + str(Jt))
            print("content cost = " + str(Jc))
            print("style cost = " + str(Js))
            
            # save current generated image in the "/output" directory
            save_image("output/" + str(i) + ".png", generated_image)
    
    # save last generated image
    save_image('output/generated_image.jpg', generated_image)
    
    return generated_image
```

## åºåˆ—æ¨¡å‹

speech recognition, natural language process

music generation,  Sentiment classification

DNA sequence analysis,  Machine translation

Video activity recognition

#### RNN

![1559184381638](E:\local\deep\deep_learning\Deep learning.assets\1559184381638.png)

ç±»å‹

![1559184423819](E:\local\deep\deep_learning\Deep learning.assets\1559184423819.png)

#### GRU(gated recurrent units)é—¨æ§å¾ªç¯å•å…ƒ

æ”¹å˜äº†RNNçš„éšè—å±‚ï¼Œ ä½¿å…¶å¯ä»¥æ›´å¥½åœ°æ•æ‰æ·±å±‚è¿æ¥ï¼Œå¹¶æ”¹å–„äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œ

Paperï¼šï¼ˆChung J, Gulcehre C, Cho K H, et al. Empirical Evaluation of Gated Recurrent Neural
Networks on Sequence Modeling[J]. Eprint Arxiv, 2014.
Cho K, Merrienboer B V, Bahdanau D, et al. On the Properties of Neural Machine Translation:
Encoder-Decoder Approaches[J]. Computer Science, 2014.ï¼‰

#### LSTM(Long short term memory)é•¿çŸ­æœŸè®°å¿†

Paperï¼šï¼ˆHochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997,
9(8):1735-1780.ï¼‰



#### NLP

use embedding 

often used Cosine Similarity
$$
sim(e_w, e_{king} - e_{man} + e_{woman})
$$

$$
sim(u,v) = \frac {u^Tv} {||u||_2||v||_2} 
$$

or use (little)
$$
||u-v||^2
$$


When you insert or overlay an "activate" clip, you will also update labels for ğ‘¦âŸ¨ğ‘¡âŸ©yâŸ¨tâŸ©, so that 50 steps of the output now have target label 1